{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eabfbc4b",
      "metadata": {},
      "source": [
        "# Timeseries Workshop Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bde2a1b",
      "metadata": {},
      "source": [
        "## Prerequisites Installation\n",
        "\n",
        "Ensure Python 3.8+ is installed\n",
        "Install required Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "975a357d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymongo in ./.venv/lib/python3.11/site-packages (4.13.0)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (3.10.3)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.3.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in ./.venv/lib/python3.11/site-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pymongo matplotlib pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48d20805",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Comparing Timeseries with Regular Collection\n",
        "**Description**: We create a regular collection and a timeseries collection with identical schema, ingest identical data, then compare storage sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68737f80",
      "metadata": {},
      "outputs": [],
      "source": [
        "MONGO_URI = \"mongodb://localhost:27017\"\n",
        "CLUSTER = \"sql-demo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "961485f9",
      "metadata": {},
      "outputs": [
        {
          "ename": "ServerSelectionTimeoutError",
          "evalue": "sql-demo-shard-00-02.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),sql-demo-shard-00-01.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),sql-demo-shard-00-00.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6841572e1e196a9a9078bcc8, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('sql-demo-shard-00-00.z2poe.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('sql-demo-shard-00-00.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('sql-demo-shard-00-01.z2poe.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('sql-demo-shard-00-01.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('sql-demo-shard-00-02.z2poe.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('sql-demo-shard-00-02.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mServerSelectionTimeoutError\u001b[39m               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m regular = db[\u001b[33m'\u001b[39m\u001b[33mregular_collection\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create timeseries collection with timeField=\"timestamp\" and metaField=\"metadata\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m ts = \u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mts_collection\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeseries\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeField\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmetaField\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgranularity\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mseconds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/_csot.py:119\u001b[39m, in \u001b[36mapply.<locals>.csot_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[32m    118\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/database.py:619\u001b[39m, in \u001b[36mDatabase.create_collection\u001b[39m\u001b[34m(self, name, codec_options, read_preference, write_concern, read_concern, session, check_exists, **kwargs)\u001b[39m\n\u001b[32m    611\u001b[39m     common.validate_is_mapping(\u001b[33m\"\u001b[39m\u001b[33mclusteredIndex\u001b[39m\u001b[33m\"\u001b[39m, clustered_index)\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client._tmp_session(session) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[32m    614\u001b[39m     \u001b[38;5;66;03m# Skip this check in a transaction where listCollections is not\u001b[39;00m\n\u001b[32m    615\u001b[39m     \u001b[38;5;66;03m# supported.\u001b[39;00m\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    617\u001b[39m         check_exists\n\u001b[32m    618\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s.in_transaction)\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_list_collection_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     ):\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CollectionInvalid(\u001b[33m\"\u001b[39m\u001b[33mcollection \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m already exists\u001b[39m\u001b[33m\"\u001b[39m % name)\n\u001b[32m    622\u001b[39m     coll = Collection(\n\u001b[32m    623\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    624\u001b[39m         name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m         read_concern,\n\u001b[32m    630\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/database.py:1191\u001b[39m, in \u001b[36mDatabase._list_collection_names\u001b[39m\u001b[34m(self, session, filter, comment, **kwargs)\u001b[39m\n\u001b[32m   1187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mfilter\u001b[39m) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m):\n\u001b[32m   1188\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnameOnly\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m     result[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_list_collections_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1192\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/database.py:1138\u001b[39m, in \u001b[36mDatabase._list_collections_helper\u001b[39m\u001b[34m(self, session, filter, comment, **kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_cmd\u001b[39m(\n\u001b[32m   1131\u001b[39m     session: Optional[ClientSession],\n\u001b[32m   1132\u001b[39m     _server: Server,\n\u001b[32m   1133\u001b[39m     conn: Connection,\n\u001b[32m   1134\u001b[39m     read_preference: _ServerMode,\n\u001b[32m   1135\u001b[39m ) -> CommandCursor[MutableMapping[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._list_collections(conn, session, read_preference=read_preference, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_retryable_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_Op\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLIST_COLLECTIONS\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:2026\u001b[39m, in \u001b[36mMongoClient._retryable_read\u001b[39m\u001b[34m(self, func, read_pref, session, operation, address, retryable, operation_id)\u001b[39m\n\u001b[32m   2021\u001b[39m \u001b[38;5;66;03m# Ensure that the client supports retrying on reads and there is no session in\u001b[39;00m\n\u001b[32m   2022\u001b[39m \u001b[38;5;66;03m# transaction, otherwise, we will not support retry behavior for this call.\u001b[39;00m\n\u001b[32m   2023\u001b[39m retryable = \u001b[38;5;28mbool\u001b[39m(\n\u001b[32m   2024\u001b[39m     retryable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.options.retry_reads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (session \u001b[38;5;129;01mand\u001b[39;00m session.in_transaction)\n\u001b[32m   2025\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_read\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m=\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/_csot.py:119\u001b[39m, in \u001b[36mapply.<locals>.csot_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[32m    118\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:1993\u001b[39m, in \u001b[36mMongoClient._retry_internal\u001b[39m\u001b[34m(self, func, session, bulk, operation, is_read, address, read_pref, retryable, operation_id)\u001b[39m\n\u001b[32m   1956\u001b[39m \u001b[38;5;129m@_csot\u001b[39m.apply\n\u001b[32m   1957\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retry_internal\u001b[39m(\n\u001b[32m   1958\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1967\u001b[39m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1968\u001b[39m ) -> T:\n\u001b[32m   1969\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Internal retryable helper for all client transactions.\u001b[39;00m\n\u001b[32m   1970\u001b[39m \n\u001b[32m   1971\u001b[39m \u001b[33;03m    :param func: Callback function we want to retry\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1980\u001b[39m \u001b[33;03m    :return: Output of the calling func()\u001b[39;00m\n\u001b[32m   1981\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1982\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ClientConnectionRetryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmongo_client\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbulk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1986\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_read\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m=\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1992\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1993\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:2730\u001b[39m, in \u001b[36m_ClientConnectionRetryable.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2728\u001b[39m \u001b[38;5;28mself\u001b[39m._check_last_error(check_csot=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2729\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_read \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write()\n\u001b[32m   2731\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[32m   2732\u001b[39m     \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[32m   2733\u001b[39m     \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[32m   2734\u001b[39m     \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n\u001b[32m   2735\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_last_error()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:2875\u001b[39m, in \u001b[36m_ClientConnectionRetryable._read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2870\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> T:\n\u001b[32m   2871\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper method for read-type retryable client executions\u001b[39;00m\n\u001b[32m   2872\u001b[39m \n\u001b[32m   2873\u001b[39m \u001b[33;03m    :return: Output for func()'s call\u001b[39;00m\n\u001b[32m   2874\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2875\u001b[39m     \u001b[38;5;28mself\u001b[39m._server = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2876\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_pref \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mRead Preference required on read calls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2877\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client._conn_from_server(\u001b[38;5;28mself\u001b[39m._read_pref, \u001b[38;5;28mself\u001b[39m._server, \u001b[38;5;28mself\u001b[39m._session) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m   2878\u001b[39m         conn,\n\u001b[32m   2879\u001b[39m         read_pref,\n\u001b[32m   2880\u001b[39m     ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:2823\u001b[39m, in \u001b[36m_ClientConnectionRetryable._get_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_server\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Server:\n\u001b[32m   2819\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Retrieves a server object based on provided object context\u001b[39;00m\n\u001b[32m   2820\u001b[39m \n\u001b[32m   2821\u001b[39m \u001b[33;03m    :return: Abstraction to connect to server\u001b[39;00m\n\u001b[32m   2822\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_server_selector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_operation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_deprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_operation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:1812\u001b[39m, in \u001b[36mMongoClient._select_server\u001b[39m\u001b[34m(self, server_selector, session, operation, address, deprioritized_servers, operation_id)\u001b[39m\n\u001b[32m   1810\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m AutoReconnect(\u001b[33m\"\u001b[39m\u001b[33mserver \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m no longer available\u001b[39m\u001b[33m\"\u001b[39m % address)  \u001b[38;5;66;03m# noqa: UP031\u001b[39;00m\n\u001b[32m   1811\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1812\u001b[39m         server = \u001b[43mtopology\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m            \u001b[49m\u001b[43mserver_selector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m            \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m            \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m server\n\u001b[32m   1819\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PyMongoError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m   1820\u001b[39m     \u001b[38;5;66;03m# Server selection errors in a transaction are transient.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/topology.py:409\u001b[39m, in \u001b[36mTopology.select_server\u001b[39m\u001b[34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_server\u001b[39m(\n\u001b[32m    400\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    401\u001b[39m     selector: Callable[[Selection], Selection],\n\u001b[32m   (...)\u001b[39m\u001b[32m    406\u001b[39m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    407\u001b[39m ) -> Server:\n\u001b[32m    408\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Like select_servers, but choose a random server if several match.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     server = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _csot.get_timeout():\n\u001b[32m    418\u001b[39m         _csot.set_rtt(server.description.min_round_trip_time)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/topology.py:387\u001b[39m, in \u001b[36mTopology._select_server\u001b[39m\u001b[34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select_server\u001b[39m(\n\u001b[32m    379\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    380\u001b[39m     selector: Callable[[Selection], Selection],\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    386\u001b[39m ) -> Server:\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     servers = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselect_servers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m     servers = _filter_servers(servers, deprioritized_servers)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(servers) == \u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/topology.py:294\u001b[39m, in \u001b[36mTopology.select_servers\u001b[39m\u001b[34m(self, selector, operation, server_selection_timeout, address, operation_id)\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m.cleanup_monitors()\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     server_descriptions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_servers_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    299\u001b[39m         cast(Server, \u001b[38;5;28mself\u001b[39m.get_server_by_address(sd.address)) \u001b[38;5;28;01mfor\u001b[39;00m sd \u001b[38;5;129;01min\u001b[39;00m server_descriptions\n\u001b[32m    300\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/oil-n-gas-timeseries/.venv/lib/python3.11/site-packages/pymongo/synchronous/topology.py:344\u001b[39m, in \u001b[36mTopology._select_servers_loop\u001b[39m\u001b[34m(self, selector, timeout, operation, operation_id, address)\u001b[39m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _SERVER_SELECTION_LOGGER.isEnabledFor(logging.DEBUG):\n\u001b[32m    334\u001b[39m         _debug_log(\n\u001b[32m    335\u001b[39m             _SERVER_SELECTION_LOGGER,\n\u001b[32m    336\u001b[39m             message=_ServerSelectionStatusMessage.FAILED,\n\u001b[32m   (...)\u001b[39m\u001b[32m    342\u001b[39m             failure=\u001b[38;5;28mself\u001b[39m._error_message(selector),\n\u001b[32m    343\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServerSelectionTimeoutError(\n\u001b[32m    345\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._error_message(selector)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Timeout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms, Topology Description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.description\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    346\u001b[39m     )\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logged_waiting:\n\u001b[32m    349\u001b[39m     _debug_log(\n\u001b[32m    350\u001b[39m         _SERVER_SELECTION_LOGGER,\n\u001b[32m    351\u001b[39m         message=_ServerSelectionStatusMessage.WAITING,\n\u001b[32m   (...)\u001b[39m\u001b[32m    357\u001b[39m         remainingTimeMS=\u001b[38;5;28mint\u001b[39m(\u001b[32m1000\u001b[39m * (end_time - time.monotonic())),\n\u001b[32m    358\u001b[39m     )\n",
            "\u001b[31mServerSelectionTimeoutError\u001b[39m: sql-demo-shard-00-02.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),sql-demo-shard-00-01.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),sql-demo-shard-00-00.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6841572e1e196a9a9078bcc8, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('sql-demo-shard-00-00.z2poe.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('sql-demo-shard-00-00.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('sql-demo-shard-00-01.z2poe.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('sql-demo-shard-00-01.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('sql-demo-shard-00-02.z2poe.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('sql-demo-shard-00-02.z2poe.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>"
          ]
        }
      ],
      "source": [
        "from pymongo import MongoClient\n",
        "import time\n",
        "import os\n",
        "\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client['timeseries_workshop']\n",
        "# Drop existing collections if present\n",
        "try:\n",
        "    db.drop_collection('regular_collection')\n",
        "    db.drop_collection('ts_collection')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create regular collection\n",
        "regular = db['regular_collection']\n",
        "# Create timeseries collection with timeField=\"timestamp\" and metaField=\"metadata\"\n",
        "ts = db.create_collection(\n",
        "    'ts_collection',\n",
        "    timeseries={\n",
        "        'timeField': 'timestamp',\n",
        "        'metaField': 'metadata',\n",
        "        'granularity': 'seconds'\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e5dc5f",
      "metadata": {},
      "source": [
        "The code above establishes a connection to a local MongoDB instance, drops any existing collections named `regular_collection` and `ts_collection`, then creates a standard collection and a new timeseries collection specifying `timestamp` as the time field and `metadata` as the metadata field. Timeseries collections in MongoDB automatically partition data by time buckets, optimizing storage and queries on time-ordered data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cad3795",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Function to generate a single sample document with well-distributed and statistically relevant values\n",
        "def generate_sample_doc(base_time):\n",
        "    # Distribute deviceId and sensorId\n",
        "    device_ids = [f'PLC-Rig{str(i).zfill(3)}' for i in range(1, 11)]\n",
        "    sensor_ids = [f'TempSensor-Bit{str(i).zfill(2)}' for i in range(1, 6)]\n",
        "    locations = [\n",
        "        {'site': 'Drilling Rig A', 'well': 'Well-XYZ', 'section': 'Drill Bit', 'latitude': 29.73656, 'longitude': -95.36980},\n",
        "        {'site': 'Drilling Rig B', 'well': 'Well-ABC', 'section': 'Pump', 'latitude': 30.12345, 'longitude': -94.56789},\n",
        "        {'site': 'Drilling Rig C', 'well': 'Well-DEF', 'section': 'Pipe', 'latitude': 28.98765, 'longitude': -96.54321}\n",
        "    ]\n",
        "    asset_metadata = [\n",
        "        {'oilField': 'Permian Basin', 'rigType': 'Land-Based', 'operator': 'OperatorCorp'},\n",
        "        {'oilField': 'Eagle Ford', 'rigType': 'Offshore', 'operator': 'DrillMasters'},\n",
        "        {'oilField': 'Bakken', 'rigType': 'Land-Based', 'operator': 'PetroWorks'}\n",
        "    ]\n",
        "    # Simulate temperature with normal distribution, some outliers\n",
        "    temp = round(random.gauss(150, 20), 2)\n",
        "    if random.random() < 0.01:\n",
        "        temp += random.choice([-40, 40])  # 1% outliers\n",
        "    # Status distribution\n",
        "    status = random.choices(['OK', 'WARN', 'FAIL'], weights=[0.85, 0.10, 0.05])[0]\n",
        "    thresholds = {\n",
        "        'lowWarning': 100.0,\n",
        "        'highWarning': 150.0,\n",
        "        'maxLimit': 175.0\n",
        "    }\n",
        "    return {\n",
        "        'timestamp': base_time,\n",
        "        'metadata': {\n",
        "            'deviceId': random.choice(device_ids),\n",
        "            'sensorId': random.choice(sensor_ids),\n",
        "            'location': random.choice(locations),\n",
        "            'assetMetadata': random.choice(asset_metadata)\n",
        "        },\n",
        "        'data': {\n",
        "            'temperature': {\n",
        "                'value': temp,\n",
        "                'unit': 'C'\n",
        "            },\n",
        "            'status': status,\n",
        "            'thresholds': thresholds\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Generate a list of 100,000 documents spaced by one second\n",
        "docs = []\n",
        "start_time = datetime.utcnow()\n",
        "for i in range(100_000):\n",
        "    docs.append(generate_sample_doc(start_time + timedelta(seconds=i)))\n",
        "\n",
        "print(f\"Generated {len(docs)} documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed4e0bdc",
      "metadata": {},
      "source": [
        "This cell defines a helper function `generate_sample_doc` that returns a document matching the provided sample schema. We generate 100,000 documents spaced one second apart starting from the current UTC time. Each document has randomized temperature data between 100°C and 200°C."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "759f498d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regular inserts for regular collection\n",
        "start = time.time()\n",
        "regular.insert_many(docs)\n",
        "elapsed_regular = time.time() - start\n",
        "\n",
        "# Timeseries inserts (same docs)\n",
        "start = time.time()\n",
        "ts.insert_many(docs)\n",
        "elapsed_ts = time.time() - start\n",
        "\n",
        "print(f\"InsertMany Regular: {elapsed_regular:.2f}s\")\n",
        "print(f\"InsertMany Timeseries: {elapsed_ts:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40d69cb1",
      "metadata": {},
      "source": [
        "We measure the time taken to insert 100,000 documents into a regular collection and a timeseries collection using `insert_many`. This highlights the throughput benefit, if any, of using timeseries collections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8331b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "stats_reg = db.command(\"collStats\", \"regular_collection\")\n",
        "stats_ts = db.command(\"collStats\", \"ts_collection\")\n",
        "size_reg = stats_reg['storageSize']\n",
        "size_ts = stats_ts['storageSize']\n",
        "print(f\"Regular Collection Storage Size: {size_reg/1024/1024:.2f} MB\")\n",
        "print(f\"Timeseries Collection Storage Size: {size_ts/1024/1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0754bc0",
      "metadata": {},
      "source": [
        "The `collStats` command returns storage statistics for each collection. We extract `storageSize` to compare the disk usage of the regular versus timeseries collection. Typically, timeseries collections use bucket compression and fewer indexes, reducing disk footprint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331e5033",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = ['Regular', 'Timeseries']\n",
        "sizes = [size_reg/1024/1024, size_ts/1024/1024]\n",
        "plt.bar(labels, sizes)\n",
        "plt.ylabel('Storage Size (MB)')\n",
        "plt.title('Regular vs Timeseries Storage Size')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96759a8c",
      "metadata": {},
      "source": [
        "We use a simple bar chart to visualize the storage size comparison. This helps quickly identify the storage efficiency offered by timeseries collections."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7345a5e",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Ingestion & Throughput Benchmarks\n",
        "**Actions**: Benchmark three insertion methods (`insert_one`, `insert_many`, and bulk operations) for timeseries collections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c6a0c86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code Cell: Prepare fresh timeseries collection for benchmark\n",
        "db.drop_collection('ts_benchmark')\n",
        "ts_bench = db.create_collection(\n",
        "    'ts_benchmark',\n",
        "    timeseries={\n",
        "        'timeField': 'timestamp',\n",
        "        'metaField': 'metadata',\n",
        "        'granularity': 'seconds'\n",
        "    }\n",
        ")\n",
        "# Generate 10,000 new documents\n",
        "docs_bench = [generate_sample_doc(start_time + timedelta(seconds=i)) for i in range(10_000)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d755be3",
      "metadata": {},
      "source": [
        "To isolate benchmarks from prior data, we drop and recreate a `ts_benchmark` collection. We generate 10,000 documents for throughput testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47e20788",
      "metadata": {},
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "for doc in docs_bench:\n",
        "    ts_bench.insert_one(doc)\n",
        "elapsed_insert_one = time.time() - start\n",
        "print(f\"InsertOne 10k documents: {elapsed_insert_one:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36931365",
      "metadata": {},
      "source": [
        "The above loop performs 10,000 `insert_one` operations, measuring the time taken. This method tends to be slower due to round-trips to the server for each document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd2099f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset collection to empty\n",
        "db.drop_collection('ts_benchmark')\n",
        "ts_bench = db.create_collection('ts_benchmark', timeseries={'timeField': 'timestamp', 'metaField': 'metadata', 'granularity': 'seconds'})\n",
        "start = time.time()\n",
        "ts_bench.insert_many(docs_bench)\n",
        "elapsed_insert_many = time.time() - start\n",
        "print(f\"InsertMany 10k documents: {elapsed_insert_many:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e40422bc",
      "metadata": {},
      "source": [
        "`insert_many` groups all documents into a single operation, reducing network overhead. We reset the collection and measure the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d014d328",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pymongo import InsertOne\n",
        "# Reset collection\n",
        "db.drop_collection('ts_benchmark')\n",
        "ts_bench = db.create_collection('ts_benchmark', timeseries={'timeField': 'timestamp', 'metaField': 'metadata', 'granularity': 'seconds'})\n",
        "requests = [InsertOne(doc) for doc in docs_bench]\n",
        "start = time.time()\n",
        "ts_bench.bulk_write(requests)\n",
        "elapsed_bulk = time.time() - start\n",
        "print(f\"BulkWrite 10k documents: {elapsed_bulk:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed8644f",
      "metadata": {},
      "outputs": [],
      "source": [
        "Using `bulk_write` with `InsertOne` operations also batches requests but allows finer control (e.g., ordering). We measure its performance here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3efbfe2",
      "metadata": {},
      "outputs": [],
      "source": [
        "methods = ['InsertOne', 'InsertMany', 'BulkWrite']\n",
        "times = [elapsed_insert_one, elapsed_insert_many, elapsed_bulk]\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(methods, times)\n",
        "plt.ylabel('Time (s)')\n",
        "plt.title('Insertion Method Throughput for 10k Docs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a3bdd38",
      "metadata": {},
      "source": [
        "A bar chart shows the relative performance: lower times indicate higher throughput. In general, `bulk_write` and `insert_many` outperform `insert_one` significantly, especially at scale."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c867b162",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Kafka Connector for Timeseries\n",
        "**Description**: Demonstrate how to configure MongoDB Kafka Connector to stream data into a timeseries collection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f871b1",
      "metadata": {},
      "source": [
        "This configuration snippet defines a Kafka Connect sink connector that reads from a Kafka topic `sensor_readings_topic` and writes into a MongoDB timeseries collection `ts_kafka`. The `timeseries.*` properties ensure the target collection is treated as a timeseries collection. To test: start Kafka, create the topic, and produce JSON messages matching the document schema."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f920f15",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Querying Timeseries Data\n",
        "### 4.1 Basic Queries\n",
        "**Description**: Query ranges of time and filter by metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "428681c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reuse docs_bench list but insert limited subset\n",
        "\n",
        "# Ensure ts_kafka exists as timeseries\n",
        "try:\n",
        "    db.drop_collection('ts_kafka')\n",
        "except:\n",
        "    pass\n",
        "kwargs = {'timeseries': {'timeField': 'timestamp', 'metaField': 'metadata', 'granularity': 'seconds'}}\n",
        "ts_kafka = db.create_collection('ts_kafka', **kwargs)\n",
        "ts_kafka.insert_many(docs_bench[:5000])  # Insert 5k docs for querying\n",
        "\n",
        "# Query: time range filter\n",
        "from datetime import datetime\n",
        "t_start = start_time + timedelta(seconds=100)\n",
        "t_end = start_time + timedelta(seconds=200)\n",
        "res = ts_kafka.find({\n",
        "    'timestamp': {'$gte': t_start, '$lte': t_end},\n",
        "    'metadata.deviceId': 'PLC-Rig123'\n",
        "})\n",
        "count = res.count()\n",
        "print(f\"Found {count} documents between {t_start} and {t_end}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4af1594",
      "metadata": {},
      "source": [
        "We create a small timeseries collection `ts_kafka`, insert 5,000 documents, and perform a basic date range query filtering by `timestamp` and `metadata.deviceId`. The output `count` shows how many documents match the criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8323c8f",
      "metadata": {},
      "source": [
        "### 4.2 Window Functions (Aggregation)\n",
        "**Description**: Use `$setWindowFields` to compute moving averages of temperature over a sliding window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c0603f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code Cell: Moving Average of Temperature over 1-minute windows\n",
        "pipeline = [\n",
        "    {'$match': {'metadata.sensorId': 'TempSensor-Bit01'}},\n",
        "    {'$setWindowFields': {\n",
        "        'partitionBy': '$metadata.sensorId',\n",
        "        'sortBy': {'timestamp': 1},\n",
        "        'output': {\n",
        "            'avgTemp': {\n",
        "                '$avg': '$data.temperature.value',\n",
        "                'window': {\n",
        "                    'range': [-60, 0],  # Last 60 seconds\n",
        "                    'unit': 'second'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }},\n",
        "    {'$limit': 5}\n",
        "]\n",
        "for doc in ts_kafka.aggregate(pipeline):\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e20ff05",
      "metadata": {},
      "source": [
        "The aggregation pipeline uses `$setWindowFields` to partition data by `sensorId`, sort by `timestamp`, and compute a moving average (`avgTemp`) over the last 60 seconds. This gives a real-time rolling metric for temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753ba34a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Retrieve full timeseries with moving average\n",
        "df = pd.DataFrame(list(ts_kafka.aggregate([\n",
        "    {'$match': {'metadata.sensorId': 'TempSensor-Bit01'}},\n",
        "    {'$setWindowFields': {\n",
        "        'partitionBy': '$metadata.sensorId',\n",
        "        'sortBy': {'timestamp': 1},\n",
        "        'output': {\n",
        "            'avgTemp': {\n",
        "                '$avg': '$data.temperature.value',\n",
        "                'window': {'range': [-60, 'unit': 'second'}, 'unit': 'second'}\n",
        "            }\n",
        "        }\n",
        "    }}\n",
        "])))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(df['timestamp'], df['data.temperature.value'], label='Instant Temp')\n",
        "plt.plot(df['timestamp'], df['avgTemp'], label='1-min Moving Avg')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Temperature (C)')\n",
        "plt.legend()\n",
        "plt.title('Temperature and 1-minute Moving Average')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c2280fd",
      "metadata": {},
      "source": [
        "We load the aggregation results into a Pandas DataFrame and plot both the instantaneous temperature and its 1-minute moving average. This visual showcases how window functions enable real-time analytics directly in MongoDB without external processing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b8c4dbe",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Realtime Analytics & Materialized Views\n",
        "### 5.1 Materialized View: Initial Creation\n",
        "**Description**: Create a materialized view that summarizes maximum temperature per minute per sensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a32af50",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop if exists\n",
        "try:\n",
        "    db.drop_collection('mv_max_temp_per_min')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create aggregation pipeline for view\n",
        "timeseries_view = [\n",
        "    {'$match': {'metadata.sensorId': 'TempSensor-Bit01'}},\n",
        "    {'$group': {\n",
        "        '_id': {\n",
        "            'sensorId': '$metadata.sensorId',\n",
        "            'minute': {\n",
        "                '$dateTrunc': {\n",
        "                    'date': '$timestamp',\n",
        "                    'unit': 'minute'\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        'maxTemp': {'$max': '$data.temperature.value'}\n",
        "    }}\n",
        "]\n",
        "# Create view\n",
        "db.create_collection(\n",
        "    'mv_max_temp_per_min',\n",
        "    viewOn='ts_kafka',\n",
        "    pipeline=timeseries_view\n",
        ")\n",
        "print(\"Materialized view 'mv_max_temp_per_min' created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc277aa",
      "metadata": {},
      "source": [
        "A view can be defined using an aggregation pipeline. Here, we create a view `mv_max_temp_per_min` that groups documents by sensor and minute, computing the maximum temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dbc1e21",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code Cell: Query the materialized view\n",
        "for doc in db['mv_max_temp_per_min'].find().limit(5):\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5504f7d",
      "metadata": {},
      "source": [
        "Querying the view returns aggregated results. In production, you could schedule an aggregation to write output to a new collection for fully materialized data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f850342",
      "metadata": {},
      "source": [
        "### 5.2 Materialized View: Updating\n",
        "**Description**: Demonstrate how to refresh materialized data by writing aggregated results to a new collection periodically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1ce31b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code Cell: Refresh materialized data\n",
        "pipeline_write = [\n",
        "    {'$match': {'metadata.sensorId': 'TempSensor-Bit01'}},\n",
        "    {'$group': {\n",
        "        '_id': {\n",
        "            'sensorId': '$metadata.sensorId',\n",
        "            'minute': {'$dateTrunc': {'date': '$timestamp', 'unit': 'minute'}}\n",
        "        },\n",
        "        'maxTemp': {'$max': '$data.temperature.value'}\n",
        "    }},\n",
        "    {'$merge': {\n",
        "        'into': 'materialized_max_temp_per_min',\n",
        "        'on': ['_id.sensorId', '_id.minute'],\n",
        "        'whenMatched': 'replace',\n",
        "        'whenNotMatched': 'insert'\n",
        "    }}\n",
        "]\n",
        "# Execute aggregation to merge into target collection\n",
        "res = ts_kafka.aggregate(pipeline_write, allowDiskUse=True)\n",
        "print(\"Materialized collection 'materialized_max_temp_per_min' updated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c8bd982",
      "metadata": {},
      "source": [
        "Using `$merge`, we write aggregated results into `materialized_max_temp_per_min`, replacing existing entries for each sensor-minute combination. In a production environment, this can run in a cron job or trigger pipeline for near real-time materialized data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad78ac9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code Cell: Verify materialized collection\n",
        "for doc in db['materialized_max_temp_per_min'].find().limit(5):\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560a578a",
      "metadata": {},
      "source": [
        "The output confirms that aggregated documents are now stored in a dedicated collection, serving as a true materialized view."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73342633",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Atlas Stream Processing\n",
        "**Description**: Illustrate setting up Atlas Stream Processing (ASP) to perform real-time transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04970e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Requires 'pymongo[srv]' and Atlas API credentials set in environment variables)\n",
        "\n",
        "from pymongo.mongo_client import MongoClient as AtlasClient\n",
        "from pymongo import InsertOne\n",
        "\n",
        "# Initialize Atlas client (replace <uri> with your Atlas connection string)\n",
        "atlas_client = AtlasClient(MONGO_URI)\n",
        "atlas_db = atlas_client['timeseries_workshop_atlas']\n",
        "\n",
        "# Define a sample ASP pipeline that triggers on ts_kafka and writes to analytics collection\n",
        "pipeline_def = {\n",
        "    'name': 'real_time_temp_processor',\n",
        "    'match': {'operationType': 'insert'},\n",
        "    'project': { 'timestamp': 1, 'data.temperature.value': 1, 'metadata.sensorId': 1 },\n",
        "    'function': \"function(event) { return { sensorId: event.fullDocument.metadata.sensorId, timestamp: event.fullDocument.timestamp, temperature: event.fullDocument.data.temperature.value }; }\",\n",
        "    'sink': {'atlasCluster': CLUSTER, 'database': 'analytics', 'collection': 'temp_events'}\n",
        "}\n",
        "\n",
        "# Create or update ASP pipeline (pseudocode; actual syntax may vary)\n",
        "atlas_db.command('createStreamPipeline', pipeline_def)\n",
        "print(\"Atlas Stream Processing pipeline 'real_time_temp_processor' configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e949011",
      "metadata": {},
      "source": [
        "The code above provides a conceptual example of defining an ASP pipeline that listens to inserts on `ts_kafka`, projects relevant fields, and writes simplified events to a target analytics collection. Replace `<username>`, `<password>`, and `<targetCluster>` with actual Atlas credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e36526c1",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Data Tiering & Online Archive\n",
        "**Description**: Show how to configure data tiering for timeseries, moving older data to an archived tier (e.g., AWS S3) and querying both in-place and archived data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c0c816",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Requires Atlas admin access and pymongo[srv]\n",
        "\n",
        "from pymongo import MongoClient\n",
        "atlas_client = MongoClient(MONGO_URI)\n",
        "policy = {\n",
        "    'collection': 'ts_kafka',\n",
        "    'database': 'timeseries_workshop',\n",
        "    'archiveName': 'ts_kafka_archive',\n",
        "    'expireAfterDays': 30,\n",
        "    'storage': {'type': 'AWS', 'bucketName': 'my-timeseries-archive-bucket', 'region': 'us-east-1'}\n",
        "}\n",
        "\n",
        "# Create Online Archive policy (actual Atlas CLI or API calls required)\n",
        "atlas_client.admin.command('configureOnlineArchive', policy)\n",
        "print(\"Online Archive policy for 'ts_kafka' configured to archive data >30 days old.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "254a1347",
      "metadata": {},
      "source": [
        "Using Atlas APIs or UI, configure an Online Archive for `ts_kafka` so that documents older than 30 days automatically move to the specified AWS S3 bucket. Queries on the collection will transparently include both live and archived data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f68c0698",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Visualization: Atlas Charts\n",
        "**Description**: Provide a conceptual guide for building charts in Atlas Charts to visualize timeseries data (no code cell executed here).\n",
        "\n",
        "1. **Connect to Data Source**: In Atlas Charts, create a new dataset pointing to the `timeseries_workshop.ts_kafka` collection.\n",
        "2. **Build a Line Chart**: Select `timestamp` on the X-axis, `data.temperature.value` on the Y-axis. Filter by `metadata.sensorId` if needed.\n",
        "3. **Add Moving Average**: Use the aggregation feature in Charts to define a 1-minute moving average pipeline (similar to `$setWindowFields`).\n",
        "4. **Dashboard Layout**: Create a dashboard combining:\n",
        "   - Current temperature gauge (use a single-value chart with `$max` aggregation on `data.temperature.value`).\n",
        "   - Time-series line chart with moving average overlay.\n",
        "   - Heatmap of temperature distribution by hour of day (aggregate by `$hour` of `timestamp`).\n",
        "\n",
        "Atlas Charts allows drag-and-drop chart building without code, leveraging MongoDB’s aggregation under the hood. Dashboards update in near-real-time for timeseries collections."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae20781d",
      "metadata": {},
      "source": [
        "---\n",
        "### High-Volume Sample Document Generator\n",
        "```python\n",
        "# Code Cell: Generate high-volume documents and write to JSON file for bulk ingestion\n",
        "import json\n",
        "\n",
        "n = 200_000  # Number of documents to generate\n",
        "base_time = datetime.utcnow()\n",
        "\n",
        "with open('high_volume_docs.json', 'w') as f:\n",
        "    for i in range(n):\n",
        "        doc = generate_sample_doc(base_time + timedelta(seconds=i))\n",
        "        f.write(json.dumps(doc) + '\\n')\n",
        "print(f\"Wrote {n} documents to high_volume_docs.json\")\n",
        "```\n",
        "\n",
        "This cell generates 200,000 sample documents and writes them to a JSON file, one document per line. Use this file with `mongoimport` for high-throughput ingestion:\n",
        "```bash\n",
        "mongoimport --uri \"mongodb://localhost:27017/timeseries_workshop\" --collection ts_bulk --file high_volume_docs.json --numInsertionWorkers 4\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55cfcef0",
      "metadata": {},
      "source": [
        "---\n",
        "### Performance Metrics Summary\n",
        "- **Storage Efficiency**: Timeseries collections typically reduce storage by ~50% using internal bucketing and compression.\n",
        "- **Insert Throughput**:\n",
        "  - `insert_one`: ~X docs/sec (dependent on hardware; for 50k docs, e.g., 5s => 10k docs/sec).\n",
        "  - `insert_many` / `bulk_write`: ~Y docs/sec (e.g., 1s => 50k docs/sec).\n",
        "- **Query Latency**: Range queries on timeseries use index on `timestamp`, yielding sub-100ms for tens of thousands of docs.\n",
        "\n",
        "Actual metrics vary by system. Always benchmark on representative infrastructure. MongoDB’s internal optimization for timeseries (bucket compression, fewer indexes) accelerates both writes and reads.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
